import asyncio
import json
import os
import random
import time
from itertools import cycle
from typing import Dict, Union, Tuple

from app.config.config import settings
from app.log.logger import get_key_manager_logger
from app.utils.helpers import redact_key_for_logging

logger = get_key_manager_logger()


class KeyManager:
    def __init__(self, api_keys: list, vertex_api_keys: list):
        self.api_keys = api_keys
        self.vertex_api_keys = vertex_api_keys
        self.key_cycle = cycle(api_keys)
        self.vertex_key_cycle = cycle(vertex_api_keys)
        self.key_cycle_lock = asyncio.Lock()
        self.vertex_key_cycle_lock = asyncio.Lock()
        self.failure_count_lock = asyncio.Lock()
        self.vertex_failure_count_lock = asyncio.Lock()
        
        # åŸæœ‰çš„å¤±è´¥è®¡æ•°å™¨
        self.key_failure_counts: Dict[str, int] = {key: 0 for key in api_keys}
        self.vertex_key_failure_counts: Dict[str, int] = {
            key: 0 for key in vertex_api_keys
        }
        
        # è´å¶æ–¯ç»Ÿè®¡æ•°æ® (alpha, beta) è¡¨ç¤º Beta(alpha, beta) åˆ†å¸ƒå‚æ•°
        self.bayesian_stats_lock = asyncio.Lock()
        self.vertex_bayesian_stats_lock = asyncio.Lock()
        self.key_stats: Dict[str, Tuple[int, int]] = {}
        self.vertex_key_stats: Dict[str, Tuple[int, int]] = {}
        
        self.MAX_FAILURES = settings.MAX_FAILURES
        self.paid_key = settings.PAID_KEY
        
        # Keyå¤æ´»æ£€æµ‹ç›¸å…³æ•°æ®ç»“æ„
        self.key_invalidation_times: Dict[str, float] = {}  # keyå¤±æ•ˆæ—¶é—´æˆ³
        self.vertex_key_invalidation_times: Dict[str, float] = {}  # vertex keyå¤±æ•ˆæ—¶é—´æˆ³
        self.resurrection_lock = asyncio.Lock()
        self.last_resurrection_check = 0.0  # ä¸Šæ¬¡å¤æ´»æ£€æµ‹æ—¶é—´
        
        # åˆå§‹åŒ–è´å¶æ–¯ç»Ÿè®¡
        self._init_bayesian_stats()

    def _init_bayesian_stats(self):
        """åˆå§‹åŒ–è´å¶æ–¯ç»Ÿè®¡æ•°æ®çš„åŸºæœ¬ç»“æ„ï¼ˆå®é™…æ•°æ®ç”±æŒä¹…åŒ–å±‚æ¢å¤ï¼‰"""
        # åˆå§‹åŒ–æ™®é€šAPI keysçš„è´å¶æ–¯ç»Ÿè®¡ï¼ˆä½¿ç”¨é»˜è®¤å€¼ï¼Œå°†è¢«æŒä¹…åŒ–å±‚è¦†ç›–ï¼‰
        for key in self.api_keys:
            # ä»ç°æœ‰å¤±è´¥è®¡æ•°è¿ç§»ä½œä¸ºé»˜è®¤å€¼: alpha=1, beta=1+failures
            alpha = settings.BAYESIAN_ALPHA_PRIOR
            beta = settings.BAYESIAN_BETA_PRIOR + self.key_failure_counts.get(key, 0)
            self.key_stats[key] = (alpha, beta)
        
        # åˆå§‹åŒ–Vertex API keysçš„è´å¶æ–¯ç»Ÿè®¡ï¼ˆä½¿ç”¨é»˜è®¤å€¼ï¼Œå°†è¢«æŒä¹…åŒ–å±‚è¦†ç›–ï¼‰
        for key in self.vertex_api_keys:
            # ä»ç°æœ‰å¤±è´¥è®¡æ•°è¿ç§»ä½œä¸ºé»˜è®¤å€¼
            alpha = settings.BAYESIAN_ALPHA_PRIOR
            beta = settings.BAYESIAN_BETA_PRIOR + self.vertex_key_failure_counts.get(key, 0)
            self.vertex_key_stats[key] = (alpha, beta)
        
        logger.debug(f"Pre-initialized Bayesian stats structure for {len(self.key_stats)} API keys and {len(self.vertex_key_stats)} Vertex keys")

    def _load_bayesian_stats(self, file_path: str) -> dict:
        """ä»æ–‡ä»¶åŠ è½½è´å¶æ–¯ç»Ÿè®¡æ•°æ®"""
        try:
            if os.path.exists(file_path):
                with open(file_path, 'r') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"Failed to load Bayesian stats from {file_path}: {e}")
        return {}

    def _save_bayesian_stats(self, file_path: str = "bayesian_key_stats.json"):
        """ä¿å­˜è´å¶æ–¯ç»Ÿè®¡æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            stats = {
                "key_stats": {k: list(v) for k, v in self.key_stats.items()},
                "vertex_key_stats": {k: list(v) for k, v in self.vertex_key_stats.items()},
                "timestamp": time.time()
            }
            with open(file_path, 'w') as f:
                json.dump(stats, f, indent=2)
            logger.debug(f"Saved Bayesian stats to {file_path}")
        except Exception as e:
            logger.error(f"Failed to save Bayesian stats to {file_path}: {e}")

    async def get_paid_key(self) -> str:
        return self.paid_key

    async def get_next_key(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ªAPI key"""
        async with self.key_cycle_lock:
            return next(self.key_cycle)

    async def get_next_vertex_key(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ª Vertex Express API key"""
        async with self.vertex_key_cycle_lock:
            return next(self.vertex_key_cycle)

    async def is_key_valid(self, key: str) -> bool:
        """æ£€æŸ¥keyæ˜¯å¦æœ‰æ•ˆ"""
        async with self.failure_count_lock:
            return self.key_failure_counts[key] < self.MAX_FAILURES

    async def is_vertex_key_valid(self, key: str) -> bool:
        """æ£€æŸ¥ Vertex key æ˜¯å¦æœ‰æ•ˆ"""
        async with self.vertex_failure_count_lock:
            return self.vertex_key_failure_counts[key] < self.MAX_FAILURES

    async def reset_failure_counts(self):
        """é‡ç½®æ‰€æœ‰keyçš„å¤±è´¥è®¡æ•°"""
        async with self.failure_count_lock:
            for key in self.key_failure_counts:
                self.key_failure_counts[key] = 0

    async def reset_vertex_failure_counts(self):
        """é‡ç½®æ‰€æœ‰ Vertex key çš„å¤±è´¥è®¡æ•°"""
        async with self.vertex_failure_count_lock:
            for key in self.vertex_key_failure_counts:
                self.vertex_key_failure_counts[key] = 0

    async def reset_key_failure_count(self, key: str) -> bool:
        """é‡ç½®æŒ‡å®škeyçš„å¤±è´¥è®¡æ•°"""
        async with self.failure_count_lock:
            if key in self.key_failure_counts:
                self.key_failure_counts[key] = 0
                logger.info(f"Reset failure count for key: {redact_key_for_logging(key)}")
                return True
            logger.warning(
                f"Attempt to reset failure count for non-existent key: {key}"
            )
            return False

    async def reset_vertex_key_failure_count(self, key: str) -> bool:
        """é‡ç½®æŒ‡å®š Vertex key çš„å¤±è´¥è®¡æ•°"""
        async with self.vertex_failure_count_lock:
            if key in self.vertex_key_failure_counts:
                self.vertex_key_failure_counts[key] = 0
                logger.info(f"Reset failure count for Vertex key: {redact_key_for_logging(key)}")
                return True
            logger.warning(
                f"Attempt to reset failure count for non-existent Vertex key: {key}"
            )
            return False

    async def get_next_working_key(self) -> str:
        """è·å–ä¸‹ä¸€å¯ç”¨çš„API key"""
        initial_key = await self.get_next_key()
        current_key = initial_key

        while True:
            if await self.is_key_valid(current_key):
                return current_key

            current_key = await self.get_next_key()
            if current_key == initial_key:
                return current_key

    async def get_next_working_vertex_key(self) -> str:
        """è·å–ä¸‹ä¸€å¯ç”¨çš„ Vertex Express API key"""
        initial_key = await self.get_next_vertex_key()
        current_key = initial_key

        while True:
            if await self.is_vertex_key_valid(current_key):
                return current_key

            current_key = await self.get_next_vertex_key()
            if current_key == initial_key:
                return current_key

    async def handle_api_failure(self, api_key: str, retries: int) -> str:
        """å¤„ç†APIè°ƒç”¨å¤±è´¥"""
        # æ›´æ–°åŸæœ‰å¤±è´¥è®¡æ•°
        async with self.failure_count_lock:
            self.key_failure_counts[api_key] += 1
            if self.key_failure_counts[api_key] >= self.MAX_FAILURES:
                logger.warning(
                    f"API key {redact_key_for_logging(api_key)} has failed {self.MAX_FAILURES} times"
                )
                # è®°å½•keyå¤±æ•ˆæ—¶é—´ï¼Œç”¨äºå¤æ´»æ£€æµ‹
                async with self.resurrection_lock:
                    self.key_invalidation_times[api_key] = time.time()
                    logger.debug(f"Recorded invalidation time for key {redact_key_for_logging(api_key)}")
        
        # æ›´æ–°è´å¶æ–¯ç»Ÿè®¡ (å¤±è´¥: beta += 1)
        await self.update_key_failure(api_key)
        
        if retries < settings.MAX_RETRIES:
            return await self.get_next_working_key()
        else:
            return ""

    async def handle_vertex_api_failure(self, api_key: str, retries: int) -> str:
        """å¤„ç† Vertex Express API è°ƒç”¨å¤±è´¥"""
        # æ›´æ–°åŸæœ‰å¤±è´¥è®¡æ•°
        async with self.vertex_failure_count_lock:
            self.vertex_key_failure_counts[api_key] += 1
            if self.vertex_key_failure_counts[api_key] >= self.MAX_FAILURES:
                logger.warning(
                    f"Vertex Express API key {redact_key_for_logging(api_key)} has failed {self.MAX_FAILURES} times"
                )
                # è®°å½•vertex keyå¤±æ•ˆæ—¶é—´ï¼Œç”¨äºå¤æ´»æ£€æµ‹
                async with self.resurrection_lock:
                    self.vertex_key_invalidation_times[api_key] = time.time()
                    logger.debug(f"Recorded invalidation time for Vertex key {redact_key_for_logging(api_key)}")
        
        # æ›´æ–°è´å¶æ–¯ç»Ÿè®¡ (å¤±è´¥: beta += 1)
        await self.update_vertex_key_failure(api_key)

    def get_fail_count(self, key: str) -> int:
        """è·å–æŒ‡å®šå¯†é’¥çš„å¤±è´¥æ¬¡æ•°"""
        return self.key_failure_counts.get(key, 0)

    def get_vertex_fail_count(self, key: str) -> int:
        """è·å–æŒ‡å®š Vertex å¯†é’¥çš„å¤±è´¥æ¬¡æ•°"""
        return self.vertex_key_failure_counts.get(key, 0)

    async def get_all_keys_with_fail_count(self) -> dict:
        """è·å–æ‰€æœ‰API keyåŠå…¶å¤±è´¥æ¬¡æ•°"""
        all_keys = {}
        async with self.failure_count_lock:
            for key in self.api_keys:
                all_keys[key] = self.key_failure_counts.get(key, 0)
        
        valid_keys = {k: v for k, v in all_keys.items() if v < self.MAX_FAILURES}
        invalid_keys = {k: v for k, v in all_keys.items() if v >= self.MAX_FAILURES}
        
        return {"valid_keys": valid_keys, "invalid_keys": invalid_keys, "all_keys": all_keys}

    async def get_keys_by_status(self) -> dict:
        """è·å–åˆ†ç±»åçš„API keyåˆ—è¡¨ï¼ŒåŒ…æ‹¬å¤±è´¥æ¬¡æ•°"""
        valid_keys = {}
        invalid_keys = {}

        async with self.failure_count_lock:
            for key in self.api_keys:
                fail_count = self.key_failure_counts[key]
                if fail_count < self.MAX_FAILURES:
                    valid_keys[key] = fail_count
                else:
                    invalid_keys[key] = fail_count

        return {"valid_keys": valid_keys, "invalid_keys": invalid_keys}

    async def get_vertex_keys_by_status(self) -> dict:
        """è·å–åˆ†ç±»åçš„ Vertex Express API key åˆ—è¡¨ï¼ŒåŒ…æ‹¬å¤±è´¥æ¬¡æ•°"""
        valid_keys = {}
        invalid_keys = {}

        async with self.vertex_failure_count_lock:
            for key in self.vertex_api_keys:
                fail_count = self.vertex_key_failure_counts[key]
                if fail_count < self.MAX_FAILURES:
                    valid_keys[key] = fail_count
                else:
                    invalid_keys[key] = fail_count
        return {"valid_keys": valid_keys, "invalid_keys": invalid_keys}

    async def get_first_valid_key(self) -> str:
        """è·å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„API key"""
        async with self.failure_count_lock:
            for key in self.key_failure_counts:
                if self.key_failure_counts[key] < self.MAX_FAILURES:
                    return key
        if self.api_keys:
            return self.api_keys[0]
        if not self.api_keys:
            logger.warning("API key list is empty, cannot get first valid key.")
            return ""
        return self.api_keys[0]

    async def get_random_valid_key(self) -> str:
        """è·å–éšæœºçš„æœ‰æ•ˆAPI key (å·²å¼ƒç”¨ï¼Œä½¿ç”¨get_bayesian_keyä»£æ›¿)"""
        logger.warning("get_random_valid_key is deprecated, using get_bayesian_key instead")
        return await self.get_bayesian_key()

    async def get_bayesian_key(self) -> str:
        """ä½¿ç”¨è´å¶æ–¯Thompson Samplingè·å–æœ€ä¼˜API key"""
        valid_keys = []
        sampled_scores = []
        
        # åŒæ—¶æ£€æŸ¥åŸæœ‰å¤±è´¥è®¡æ•°å’Œè´å¶æ–¯ç»Ÿè®¡ï¼Œç¡®ä¿ä¸€è‡´æ€§
        async with self.failure_count_lock:
            async with self.bayesian_stats_lock:
                for key in self.api_keys:
                    # ä½¿ç”¨åŸæœ‰çš„å¤±è´¥è®¡æ•°ä½œä¸ºä¸»è¦åˆ¤æ–­æ ‡å‡†
                    original_failure_count = self.key_failure_counts.get(key, 0)
                    
                    # æ£€æŸ¥keyæ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    if original_failure_count < self.MAX_FAILURES:
                        if key in self.key_stats:
                            alpha, beta = self.key_stats[key]
                            # åŒé‡éªŒè¯ï¼šä¹Ÿæ£€æŸ¥è´å¶æ–¯ç»Ÿè®¡çš„å¤±è´¥æ¬¡æ•°
                            bayesian_failure_count = beta - settings.BAYESIAN_BETA_PRIOR
                            
                            # å¦‚æœä¸¤ä¸ªè®¡æ•°ä¸ä¸€è‡´ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­ä½¿ç”¨åŸæœ‰è®¡æ•°
                            if abs(bayesian_failure_count - original_failure_count) > 1:
                                logger.warning(f"Key {redact_key_for_logging(key)} has inconsistent failure counts: "
                                             f"original={original_failure_count}, bayesian={bayesian_failure_count}")
                            
                            valid_keys.append(key)
                            # Thompson Sampling: ä»Betaåˆ†å¸ƒé‡‡æ ·
                            try:
                                if alpha > 0 and beta > 0:
                                    sampled_prob = random.betavariate(alpha, beta)
                                else:
                                    # é˜²æ­¢å‚æ•°å¼‚å¸¸ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
                                    sampled_prob = random.random()
                                sampled_scores.append(sampled_prob)
                            except ValueError:
                                # betavariateå‚æ•°å¼‚å¸¸æ—¶çš„fallback
                                sampled_scores.append(random.random())
                        else:
                            # å¦‚æœkeyä¸åœ¨è´å¶æ–¯ç»Ÿè®¡ä¸­ï¼Œç»™äºˆé»˜è®¤æ¦‚ç‡ä½†ä»ç„¶åŒ…å«
                            valid_keys.append(key)
                            sampled_scores.append(0.5)
                            logger.debug(f"Key {redact_key_for_logging(key)} not in Bayesian stats, using default probability")
        
        if valid_keys and sampled_scores:
            # é€‰æ‹©é‡‡æ ·æ¦‚ç‡æœ€é«˜çš„key
            best_idx = sampled_scores.index(max(sampled_scores))
            selected_key = valid_keys[best_idx]
            logger.debug(f"Bayesian selection: chose key {redact_key_for_logging(selected_key)} "
                        f"with sampled prob {sampled_scores[best_idx]:.4f}")
            return selected_key
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„keyï¼Œè¿”å›ç¬¬ä¸€ä¸ªkeyä½œä¸ºfallback
        if self.api_keys:
            logger.warning("No valid keys available for Bayesian selection, returning first key as fallback.")
            return self.api_keys[0]
        
        logger.warning("API key list is empty, cannot perform Bayesian key selection.")
        return ""

    async def get_bayesian_vertex_key(self) -> str:
        """ä½¿ç”¨è´å¶æ–¯Thompson Samplingè·å–æœ€ä¼˜Vertex API key"""
        valid_keys = []
        sampled_scores = []
        
        # åŒæ—¶æ£€æŸ¥åŸæœ‰å¤±è´¥è®¡æ•°å’Œè´å¶æ–¯ç»Ÿè®¡ï¼Œç¡®ä¿ä¸€è‡´æ€§
        async with self.vertex_failure_count_lock:
            async with self.vertex_bayesian_stats_lock:
                for key in self.vertex_api_keys:
                    # ä½¿ç”¨åŸæœ‰çš„å¤±è´¥è®¡æ•°ä½œä¸ºä¸»è¦åˆ¤æ–­æ ‡å‡†
                    original_failure_count = self.vertex_key_failure_counts.get(key, 0)
                    
                    # æ£€æŸ¥keyæ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    if original_failure_count < self.MAX_FAILURES:
                        if key in self.vertex_key_stats:
                            alpha, beta = self.vertex_key_stats[key]
                            # åŒé‡éªŒè¯ï¼šä¹Ÿæ£€æŸ¥è´å¶æ–¯ç»Ÿè®¡çš„å¤±è´¥æ¬¡æ•°
                            bayesian_failure_count = beta - settings.BAYESIAN_BETA_PRIOR
                            
                            # å¦‚æœä¸¤ä¸ªè®¡æ•°ä¸ä¸€è‡´ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­ä½¿ç”¨åŸæœ‰è®¡æ•°
                            if abs(bayesian_failure_count - original_failure_count) > 1:
                                logger.warning(f"Vertex key {redact_key_for_logging(key)} has inconsistent failure counts: "
                                             f"original={original_failure_count}, bayesian={bayesian_failure_count}")
                            
                            valid_keys.append(key)
                            # Thompson Sampling: ä»Betaåˆ†å¸ƒé‡‡æ ·
                            try:
                                if alpha > 0 and beta > 0:
                                    sampled_prob = random.betavariate(alpha, beta)
                                else:
                                    sampled_prob = random.random()
                                sampled_scores.append(sampled_prob)
                            except ValueError:
                                sampled_scores.append(random.random())
                        else:
                            # å¦‚æœkeyä¸åœ¨è´å¶æ–¯ç»Ÿè®¡ä¸­ï¼Œç»™äºˆé»˜è®¤æ¦‚ç‡ä½†ä»ç„¶åŒ…å«
                            valid_keys.append(key)
                            sampled_scores.append(0.5)
                            logger.debug(f"Vertex key {redact_key_for_logging(key)} not in Bayesian stats, using default probability")
        
        if valid_keys and sampled_scores:
            # é€‰æ‹©é‡‡æ ·æ¦‚ç‡æœ€é«˜çš„key
            best_idx = sampled_scores.index(max(sampled_scores))
            selected_key = valid_keys[best_idx]
            logger.debug(f"Bayesian Vertex selection: chose key {redact_key_for_logging(selected_key)} "
                        f"with sampled prob {sampled_scores[best_idx]:.4f}")
            return selected_key
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„keyï¼Œè¿”å›ç¬¬ä¸€ä¸ªkeyä½œä¸ºfallback
        if self.vertex_api_keys:
            logger.warning("No valid Vertex keys available for Bayesian selection, returning first key as fallback.")
            return self.vertex_api_keys[0]
        
        logger.warning("Vertex API key list is empty, cannot perform Bayesian key selection.")
        return ""

    async def update_key_success(self, api_key: str):
        """æ›´æ–°API keyæˆåŠŸç»Ÿè®¡ (alpha += 1)"""
        async with self.bayesian_stats_lock:
            if api_key in self.key_stats:
                alpha, beta = self.key_stats[api_key]
                self.key_stats[api_key] = (alpha + 1, beta)
                logger.debug(f"Updated success for key {redact_key_for_logging(api_key)}: alpha={alpha+1}, beta={beta}")
            else:
                # å¦‚æœkeyä¸åœ¨ç»Ÿè®¡ä¸­ï¼Œåˆå§‹åŒ–
                self.key_stats[api_key] = (settings.BAYESIAN_ALPHA_PRIOR + 1, settings.BAYESIAN_BETA_PRIOR)
                logger.debug(f"Initialized and updated success for new key {redact_key_for_logging(api_key)}")
        
        # è§¦å‘å‘¨æœŸæ€§ä¿å­˜
        await _auto_save_bayesian_stats(self)

    async def update_key_failure(self, api_key: str):
        """æ›´æ–°API keyå¤±è´¥ç»Ÿè®¡ (beta += 1)"""
        async with self.bayesian_stats_lock:
            if api_key in self.key_stats:
                alpha, beta = self.key_stats[api_key]
                self.key_stats[api_key] = (alpha, beta + 1)
                logger.debug(f"Updated failure for key {redact_key_for_logging(api_key)}: alpha={alpha}, beta={beta+1}")
            else:
                # å¦‚æœkeyä¸åœ¨ç»Ÿè®¡ä¸­ï¼Œåˆå§‹åŒ–
                self.key_stats[api_key] = (settings.BAYESIAN_ALPHA_PRIOR, settings.BAYESIAN_BETA_PRIOR + 1)
                logger.debug(f"Initialized and updated failure for new key {redact_key_for_logging(api_key)}")
        
        # è§¦å‘å‘¨æœŸæ€§ä¿å­˜
        await _auto_save_bayesian_stats(self)

    async def update_vertex_key_success(self, api_key: str):
        """æ›´æ–°Vertex API keyæˆåŠŸç»Ÿè®¡ (alpha += 1)"""
        async with self.vertex_bayesian_stats_lock:
            if api_key in self.vertex_key_stats:
                alpha, beta = self.vertex_key_stats[api_key]
                self.vertex_key_stats[api_key] = (alpha + 1, beta)
                logger.debug(f"Updated Vertex success for key {redact_key_for_logging(api_key)}: alpha={alpha+1}, beta={beta}")
            else:
                # å¦‚æœkeyä¸åœ¨ç»Ÿè®¡ä¸­ï¼Œåˆå§‹åŒ–
                self.vertex_key_stats[api_key] = (settings.BAYESIAN_ALPHA_PRIOR + 1, settings.BAYESIAN_BETA_PRIOR)
                logger.debug(f"Initialized and updated Vertex success for new key {redact_key_for_logging(api_key)}")
        
        # è§¦å‘å‘¨æœŸæ€§ä¿å­˜
        await _auto_save_bayesian_stats(self)

    async def update_vertex_key_failure(self, api_key: str):
        """æ›´æ–°Vertex API keyå¤±è´¥ç»Ÿè®¡ (beta += 1)"""
        async with self.vertex_bayesian_stats_lock:
            if api_key in self.vertex_key_stats:
                alpha, beta = self.vertex_key_stats[api_key]
                self.vertex_key_stats[api_key] = (alpha, beta + 1)
                logger.debug(f"Updated Vertex failure for key {redact_key_for_logging(api_key)}: alpha={alpha}, beta={beta+1}")
            else:
                # å¦‚æœkeyä¸åœ¨ç»Ÿè®¡ä¸­ï¼Œåˆå§‹åŒ–
                self.vertex_key_stats[api_key] = (settings.BAYESIAN_ALPHA_PRIOR, settings.BAYESIAN_BETA_PRIOR + 1)
                logger.debug(f"Initialized and updated Vertex failure for new key {redact_key_for_logging(api_key)}")
        
        # è§¦å‘å‘¨æœŸæ€§ä¿å­˜
        await _auto_save_bayesian_stats(self)
    
    async def sync_failure_counts(self):
        """åŒæ­¥åŸæœ‰å¤±è´¥è®¡æ•°å’Œè´å¶æ–¯ç»Ÿè®¡ï¼Œç¡®ä¿ä¸€è‡´æ€§"""
        async with self.failure_count_lock:
            async with self.bayesian_stats_lock:
                # åŒæ­¥æ™®é€šAPI keys
                for key in self.api_keys:
                    if key in self.key_stats:
                        alpha, beta = self.key_stats[key]
                        expected_failures = beta - settings.BAYESIAN_BETA_PRIOR
                        actual_failures = self.key_failure_counts.get(key, 0)
                        
                        if expected_failures != actual_failures:
                            logger.info(f"Syncing failure counts for key {redact_key_for_logging(key)}: "
                                       f"original={actual_failures} -> bayesian={expected_failures}")
                            # ä»¥è´å¶æ–¯ç»Ÿè®¡ä¸ºå‡†ï¼Œæ›´æ–°åŸæœ‰è®¡æ•°
                            self.key_failure_counts[key] = max(0, expected_failures)
        
        async with self.vertex_failure_count_lock:
            async with self.vertex_bayesian_stats_lock:
                # åŒæ­¥Vertex API keys
                for key in self.vertex_api_keys:
                    if key in self.vertex_key_stats:
                        alpha, beta = self.vertex_key_stats[key]
                        expected_failures = beta - settings.BAYESIAN_BETA_PRIOR
                        actual_failures = self.vertex_key_failure_counts.get(key, 0)
                        
                        if expected_failures != actual_failures:
                            logger.info(f"Syncing failure counts for Vertex key {redact_key_for_logging(key)}: "
                                       f"original={actual_failures} -> bayesian={expected_failures}")
                            # ä»¥è´å¶æ–¯ç»Ÿè®¡ä¸ºå‡†ï¼Œæ›´æ–°åŸæœ‰è®¡æ•°
                            self.vertex_key_failure_counts[key] = max(0, expected_failures)
    
    async def check_key_resurrection(self):
        """æ£€æŸ¥å¹¶å°è¯•å¤æ´»å¤±æ•ˆçš„keys"""
        if not settings.KEY_RESURRECTION_ENABLED:
            return
        
        current_time = time.time()
        
        # æ£€æŸ¥æ˜¯å¦åˆ°äº†å¤æ´»æ£€æµ‹æ—¶é—´
        if current_time - self.last_resurrection_check < settings.KEY_RESURRECTION_INTERVAL:
            return
        
        self.last_resurrection_check = current_time
        logger.debug("Starting key resurrection check")
        
        # æ£€æŸ¥æ™®é€šAPI keys
        await self._check_invalid_keys_resurrection(is_vertex=False)
        
        # æ£€æŸ¥Vertex API keys  
        await self._check_invalid_keys_resurrection(is_vertex=True)
        
        logger.debug("Completed key resurrection check")
    
    async def _check_invalid_keys_resurrection(self, is_vertex: bool = False):
        """æ£€æŸ¥ç‰¹å®šç±»å‹çš„æ— æ•ˆkeysæ˜¯å¦å¯ä»¥å¤æ´»"""
        current_time = time.time()
        keys_to_test = []
        
        async with self.resurrection_lock:
            if is_vertex:
                invalidation_times = self.vertex_key_invalidation_times
                failure_counts = self.vertex_key_failure_counts
                lock = self.vertex_failure_count_lock
            else:
                invalidation_times = self.key_invalidation_times
                failure_counts = self.key_failure_counts
                lock = self.failure_count_lock
            
            # æ‰¾å‡ºå·²è¿‡å†·å´æœŸçš„å¤±æ•ˆkeys
            for key, invalidation_time in invalidation_times.items():
                if current_time - invalidation_time >= settings.KEY_RESURRECTION_COOLDOWN:
                    async with lock:
                        if failure_counts.get(key, 0) >= self.MAX_FAILURES:
                            keys_to_test.append(key)
        
        # æµ‹è¯•æ¯ä¸ªå€™é€‰key
        for key in keys_to_test:
            success = await self._test_key_validity(key, is_vertex)
            if success:
                await self._resurrect_key(key, is_vertex)
    
    async def _test_key_validity(self, api_key: str, is_vertex: bool = False) -> bool:
        """è½»é‡çº§æµ‹è¯•keyæ˜¯å¦å·²æ¢å¤æœ‰æ•ˆæ€§"""
        try:
            if is_vertex:
                # ä¸ºVertex keyåˆ›å»ºç®€å•çš„æµ‹è¯•è¯·æ±‚
                test_client = None  # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„Vertex APIå®¢æˆ·ç«¯å®ç°
                logger.debug(f"Testing Vertex key validity: {redact_key_for_logging(api_key)}")
                # ç®€åŒ–å®ç°ï¼šå‡è®¾Vertex keyæµ‹è¯•
                return False  # æš‚æ—¶è¿”å›Falseï¼Œéœ€è¦å®é™…çš„Vertex APIæµ‹è¯•é€»è¾‘
            else:
                # ä¸ºæ™®é€šAPI keyåˆ›å»ºç®€å•çš„æµ‹è¯•è¯·æ±‚
                from app.service.client.api_client import GeminiApiClient
                test_client = GeminiApiClient(settings.BASE_URL, 10)  # 10ç§’è¶…æ—¶
                
                # åˆ›å»ºè½»é‡çº§æµ‹è¯•è¯·æ±‚
                test_payload = {
                    "contents": [{"parts": [{"text": "test"}]}],
                    "generationConfig": {"maxOutputTokens": 1}
                }
                
                logger.debug(f"Testing key validity: {redact_key_for_logging(api_key)}")
                await test_client.generate_content(test_payload, settings.KEY_RESURRECTION_TEST_MODEL, api_key)
                return True
                
        except Exception as e:
            logger.debug(f"Key {redact_key_for_logging(api_key)} still invalid: {str(e)}")
            return False
    
    async def _resurrect_key(self, api_key: str, is_vertex: bool = False):
        """å¤æ´»ä¸€ä¸ªkeyï¼Œé‡ç½®å…¶ç»Ÿè®¡æ•°æ®"""
        try:
            if is_vertex:
                async with self.vertex_failure_count_lock:
                    self.vertex_key_failure_counts[api_key] = 0
                
                async with self.vertex_bayesian_stats_lock:
                    if api_key in self.vertex_key_stats:
                        alpha, _ = self.vertex_key_stats[api_key]
                        # é‡ç½®betaä¸ºå…ˆéªŒå€¼ï¼Œä¿ç•™æˆåŠŸç»éªŒ(alpha)
                        self.vertex_key_stats[api_key] = (alpha, settings.BAYESIAN_BETA_PRIOR)
                
                async with self.resurrection_lock:
                    if api_key in self.vertex_key_invalidation_times:
                        del self.vertex_key_invalidation_times[api_key]
                
                logger.info(f"Resurrected Vertex key: {redact_key_for_logging(api_key)}")
            else:
                async with self.failure_count_lock:
                    self.key_failure_counts[api_key] = 0
                
                async with self.bayesian_stats_lock:
                    if api_key in self.key_stats:
                        alpha, _ = self.key_stats[api_key]
                        # é‡ç½®betaä¸ºå…ˆéªŒå€¼ï¼Œä¿ç•™æˆåŠŸç»éªŒ(alpha)
                        self.key_stats[api_key] = (alpha, settings.BAYESIAN_BETA_PRIOR)
                
                async with self.resurrection_lock:
                    if api_key in self.key_invalidation_times:
                        del self.key_invalidation_times[api_key]
                
                logger.info(f"Resurrected key: {redact_key_for_logging(api_key)}")
            
            # è§¦å‘ç»Ÿè®¡ä¿å­˜
            await _auto_save_bayesian_stats(self)
            
        except Exception as e:
            logger.error(f"Error resurrecting key {redact_key_for_logging(api_key)}: {e}")


_singleton_instance = None
_singleton_lock = asyncio.Lock()
_preserved_failure_counts: Union[Dict[str, int], None] = None
_preserved_vertex_failure_counts: Union[Dict[str, int], None] = None
_preserved_old_api_keys_for_reset: Union[list, None] = None
_preserved_vertex_old_api_keys_for_reset: Union[list, None] = None
_preserved_next_key_in_cycle: Union[str, None] = None
_preserved_vertex_next_key_in_cycle: Union[str, None] = None

# è´å¶æ–¯ç»Ÿè®¡çš„å…¨å±€ä¿å­˜å˜é‡
_preserved_bayesian_stats: Union[Dict[str, Tuple[int, int]], None] = None
_preserved_vertex_bayesian_stats: Union[Dict[str, Tuple[int, int]], None] = None
_last_stats_save_time: float = 0


async def get_key_manager_instance(
    api_keys: list = None, vertex_api_keys: list = None
) -> KeyManager:
    """
    è·å– KeyManager å•ä¾‹å®ä¾‹ã€‚

    å¦‚æœå°šæœªåˆ›å»ºå®ä¾‹ï¼Œå°†ä½¿ç”¨æä¾›çš„ api_keys,vertex_api_keys åˆå§‹åŒ– KeyManagerã€‚
    å¦‚æœå·²åˆ›å»ºå®ä¾‹ï¼Œåˆ™å¿½ç•¥ api_keys å‚æ•°ï¼Œè¿”å›ç°æœ‰å•ä¾‹ã€‚
    å¦‚æœåœ¨é‡ç½®åè°ƒç”¨ï¼Œä¼šå°è¯•æ¢å¤ä¹‹å‰çš„çŠ¶æ€ï¼ˆå¤±è´¥è®¡æ•°ã€å¾ªç¯ä½ç½®ï¼‰ã€‚
    """
    global _singleton_instance, _preserved_failure_counts, _preserved_vertex_failure_counts, _preserved_old_api_keys_for_reset, _preserved_vertex_old_api_keys_for_reset, _preserved_next_key_in_cycle, _preserved_vertex_next_key_in_cycle

    async with _singleton_lock:
        if _singleton_instance is None:
            if api_keys is None:
                raise ValueError(
                    "API keys are required to initialize or re-initialize the KeyManager instance."
                )
            if vertex_api_keys is None:
                raise ValueError(
                    "Vertex Express API keys are required to initialize or re-initialize the KeyManager instance."
                )

            if not api_keys:
                logger.warning(
                    "Initializing KeyManager with an empty list of API keys."
                )
            if not vertex_api_keys:
                logger.warning(
                    "Initializing KeyManager with an empty list of Vertex Express API keys."
                )

            _singleton_instance = KeyManager(api_keys, vertex_api_keys)
            logger.info(
                f"KeyManager instance created/re-created with {len(api_keys)} API keys and {len(vertex_api_keys)} Vertex Express API keys."
            )
            
            # æ¢å¤è´å¶æ–¯ç»Ÿè®¡
            await _restore_bayesian_stats(_singleton_instance)
            
            # åŒæ­¥å¤±è´¥è®¡æ•°ç¡®ä¿ä¸€è‡´æ€§
            await _singleton_instance.sync_failure_counts()

            # 1. æ¢å¤å¤±è´¥è®¡æ•°
            if _preserved_failure_counts:
                current_failure_counts = {
                    key: 0 for key in _singleton_instance.api_keys
                }
                for key, count in _preserved_failure_counts.items():
                    if key in current_failure_counts:
                        current_failure_counts[key] = count
                _singleton_instance.key_failure_counts = current_failure_counts
                logger.info("Inherited failure counts for applicable keys.")
            _preserved_failure_counts = None

            if _preserved_vertex_failure_counts:
                current_vertex_failure_counts = {
                    key: 0 for key in _singleton_instance.vertex_api_keys
                }
                for key, count in _preserved_vertex_failure_counts.items():
                    if key in current_vertex_failure_counts:
                        current_vertex_failure_counts[key] = count
                _singleton_instance.vertex_key_failure_counts = (
                    current_vertex_failure_counts
                )
                logger.info("Inherited failure counts for applicable Vertex keys.")
            _preserved_vertex_failure_counts = None

            # 2. è°ƒæ•´ key_cycle çš„èµ·å§‹ç‚¹
            start_key_for_new_cycle = None
            if (
                _preserved_old_api_keys_for_reset
                and _preserved_next_key_in_cycle
                and _singleton_instance.api_keys
            ):
                try:
                    start_idx_in_old = _preserved_old_api_keys_for_reset.index(
                        _preserved_next_key_in_cycle
                    )

                    for i in range(len(_preserved_old_api_keys_for_reset)):
                        current_old_key_idx = (start_idx_in_old + i) % len(
                            _preserved_old_api_keys_for_reset
                        )
                        key_candidate = _preserved_old_api_keys_for_reset[
                            current_old_key_idx
                        ]
                        if key_candidate in _singleton_instance.api_keys:
                            start_key_for_new_cycle = key_candidate
                            break
                except ValueError:
                    logger.warning(
                        f"Preserved next key '{_preserved_next_key_in_cycle}' not found in preserved old API keys. "
                        "New cycle will start from the beginning of the new list."
                    )
                except Exception as e:
                    logger.error(
                        f"Error determining start key for new cycle from preserved state: {e}. "
                        "New cycle will start from the beginning."
                    )

            if start_key_for_new_cycle and _singleton_instance.api_keys:
                try:
                    target_idx = _singleton_instance.api_keys.index(
                        start_key_for_new_cycle
                    )
                    for _ in range(target_idx):
                        next(_singleton_instance.key_cycle)
                    logger.info(
                        f"Key cycle in new instance advanced. Next call to get_next_key() will yield: {start_key_for_new_cycle}"
                    )
                except ValueError:
                    logger.warning(
                        f"Determined start key '{start_key_for_new_cycle}' not found in new API keys during cycle advancement. "
                        "New cycle will start from the beginning."
                    )
                except StopIteration:
                    logger.error(
                        "StopIteration while advancing key cycle, implies empty new API key list previously missed."
                    )
                except Exception as e:
                    logger.error(
                        f"Error advancing new key cycle: {e}. Cycle will start from beginning."
                    )
            else:
                if _singleton_instance.api_keys:
                    logger.info(
                        "New key cycle will start from the beginning of the new API key list (no specific start key determined or needed)."
                    )
                else:
                    logger.info(
                        "New key cycle not applicable as the new API key list is empty."
                    )

            # æ¸…ç†æ‰€æœ‰ä¿å­˜çš„çŠ¶æ€
            _preserved_old_api_keys_for_reset = None
            _preserved_next_key_in_cycle = None

            # 3. è°ƒæ•´ vertex_key_cycle çš„èµ·å§‹ç‚¹
            start_key_for_new_vertex_cycle = None
            if (
                _preserved_vertex_old_api_keys_for_reset
                and _preserved_vertex_next_key_in_cycle
                and _singleton_instance.vertex_api_keys
            ):
                try:
                    start_idx_in_old = _preserved_vertex_old_api_keys_for_reset.index(
                        _preserved_vertex_next_key_in_cycle
                    )

                    for i in range(len(_preserved_vertex_old_api_keys_for_reset)):
                        current_old_key_idx = (start_idx_in_old + i) % len(
                            _preserved_vertex_old_api_keys_for_reset
                        )
                        key_candidate = _preserved_vertex_old_api_keys_for_reset[
                            current_old_key_idx
                        ]
                        if key_candidate in _singleton_instance.vertex_api_keys:
                            start_key_for_new_vertex_cycle = key_candidate
                            break
                except ValueError:
                    logger.warning(
                        f"Preserved next key '{_preserved_vertex_next_key_in_cycle}' not found in preserved old Vertex Express API keys. "
                        "New cycle will start from the beginning of the new list."
                    )
                except Exception as e:
                    logger.error(
                        f"Error determining start key for new Vertex key cycle from preserved state: {e}. "
                        "New cycle will start from the beginning."
                    )

            if start_key_for_new_vertex_cycle and _singleton_instance.vertex_api_keys:
                try:
                    target_idx = _singleton_instance.vertex_api_keys.index(
                        start_key_for_new_vertex_cycle
                    )
                    for _ in range(target_idx):
                        next(_singleton_instance.vertex_key_cycle)
                    logger.info(
                        f"Vertex key cycle in new instance advanced. Next call to get_next_vertex_key() will yield: {start_key_for_new_vertex_cycle}"
                    )
                except ValueError:
                    logger.warning(
                        f"Determined start key '{start_key_for_new_vertex_cycle}' not found in new Vertex Express API keys during cycle advancement. "
                        "New cycle will start from the beginning."
                    )
                except StopIteration:
                    logger.error(
                        "StopIteration while advancing Vertex key cycle, implies empty new Vertex Express API key list previously missed."
                    )
                except Exception as e:
                    logger.error(
                        f"Error advancing new Vertex key cycle: {e}. Cycle will start from beginning."
                    )
            else:
                if _singleton_instance.vertex_api_keys:
                    logger.info(
                        "New Vertex key cycle will start from the beginning of the new Vertex Express API key list (no specific start key determined or needed)."
                    )
                else:
                    logger.info(
                        "New Vertex key cycle not applicable as the new Vertex Express API key list is empty."
                    )

            # æ¸…ç†æ‰€æœ‰ä¿å­˜çš„çŠ¶æ€
            _preserved_vertex_old_api_keys_for_reset = None
            _preserved_vertex_next_key_in_cycle = None

        # å®šæœŸä¿å­˜è´å¶æ–¯ç»Ÿè®¡
        await _auto_save_bayesian_stats(_singleton_instance)
        
        # æ£€æŸ¥keyå¤æ´»
        await _singleton_instance.check_key_resurrection()
        
        return _singleton_instance


async def reset_key_manager_instance():
    """
    é‡ç½® KeyManager å•ä¾‹å®ä¾‹ã€‚
    å°†ä¿å­˜å½“å‰å®ä¾‹çš„çŠ¶æ€ï¼ˆå¤±è´¥è®¡æ•°ã€æ—§ API keysã€ä¸‹ä¸€ä¸ª key æç¤ºï¼‰
    ä»¥ä¾›ä¸‹ä¸€æ¬¡ get_key_manager_instance è°ƒç”¨æ—¶æ¢å¤ã€‚
    """
    global _singleton_instance, _preserved_failure_counts, _preserved_vertex_failure_counts, _preserved_old_api_keys_for_reset, _preserved_vertex_old_api_keys_for_reset, _preserved_next_key_in_cycle, _preserved_vertex_next_key_in_cycle, _preserved_bayesian_stats, _preserved_vertex_bayesian_stats
    async with _singleton_lock:
        if _singleton_instance:
            # 0. ä¿å­˜è´å¶æ–¯ç»Ÿè®¡
            async with _singleton_instance.bayesian_stats_lock:
                _preserved_bayesian_stats = _singleton_instance.key_stats.copy()
            async with _singleton_instance.vertex_bayesian_stats_lock:
                _preserved_vertex_bayesian_stats = _singleton_instance.vertex_key_stats.copy()
            
            # 1. ä¿å­˜å¤±è´¥è®¡æ•°
            _preserved_failure_counts = _singleton_instance.key_failure_counts.copy()
            _preserved_vertex_failure_counts = (
                _singleton_instance.vertex_key_failure_counts.copy()
            )

            # 2. ä¿å­˜æ—§çš„ API keys åˆ—è¡¨
            _preserved_old_api_keys_for_reset = _singleton_instance.api_keys.copy()
            _preserved_vertex_old_api_keys_for_reset = (
                _singleton_instance.vertex_api_keys.copy()
            )

            # 3. ä¿å­˜ key_cycle çš„ä¸‹ä¸€ä¸ª key æç¤º
            try:
                if _singleton_instance.api_keys:
                    _preserved_next_key_in_cycle = (
                        await _singleton_instance.get_next_key()
                    )
                else:
                    _preserved_next_key_in_cycle = None
            except StopIteration:
                logger.warning(
                    "Could not preserve next key hint: key cycle was empty or exhausted in old instance."
                )
                _preserved_next_key_in_cycle = None
            except Exception as e:
                logger.error(f"Error preserving next key hint during reset: {e}")
                _preserved_next_key_in_cycle = None

            # 4. ä¿å­˜ vertex_key_cycle çš„ä¸‹ä¸€ä¸ª key æç¤º
            try:
                if _singleton_instance.vertex_api_keys:
                    _preserved_vertex_next_key_in_cycle = (
                        await _singleton_instance.get_next_vertex_key()
                    )
                else:
                    _preserved_vertex_next_key_in_cycle = None
            except StopIteration:
                logger.warning(
                    "Could not preserve next key hint: Vertex key cycle was empty or exhausted in old instance."
                )
                _preserved_vertex_next_key_in_cycle = None
            except Exception as e:
                logger.error(f"Error preserving next key hint during reset: {e}")
                _preserved_vertex_next_key_in_cycle = None

            _singleton_instance = None
            logger.info(
                "KeyManager instance has been reset. State (failure counts, old keys, next key hint) preserved for next instantiation."
            )
        else:
            logger.info(
                "KeyManager instance was not set (or already reset), no reset action performed."
            )


async def _restore_bayesian_stats(instance: KeyManager):
    """æ¢å¤è´å¶æ–¯ç»Ÿè®¡æ•°æ®åˆ°KeyManagerå®ä¾‹"""
    global _preserved_bayesian_stats, _preserved_vertex_bayesian_stats
    
    try:
        # æ ‡è®°æ˜¯å¦æœ‰å…¨å±€çŠ¶æ€æ¢å¤
        has_preserved_state = bool(_preserved_bayesian_stats or _preserved_vertex_bayesian_stats)
        
        # é¦–å…ˆå°è¯•ä»å…¨å±€å˜é‡æ¢å¤ï¼ˆé‡ç½®åçš„çŠ¶æ€ï¼‰
        if _preserved_bayesian_stats:
            async with instance.bayesian_stats_lock:
                for key in instance.api_keys:
                    if key in _preserved_bayesian_stats:
                        instance.key_stats[key] = _preserved_bayesian_stats[key]
            logger.info(f"Restored Bayesian stats for {len(_preserved_bayesian_stats)} API keys from preserved state")
            _preserved_bayesian_stats = None
        
        if _preserved_vertex_bayesian_stats:
            async with instance.vertex_bayesian_stats_lock:
                for key in instance.vertex_api_keys:
                    if key in _preserved_vertex_bayesian_stats:
                        instance.vertex_key_stats[key] = _preserved_vertex_bayesian_stats[key]
            logger.info(f"Restored Vertex Bayesian stats for {len(_preserved_vertex_bayesian_stats)} Vertex keys from preserved state")
            _preserved_vertex_bayesian_stats = None
        
        # å¦‚æœæ²¡æœ‰å…¨å±€çŠ¶æ€ï¼Œå°è¯•ä»æ–‡ä»¶åŠ è½½
        if not has_preserved_state:
            loaded_stats = instance._load_bayesian_stats("bayesian_key_stats.json")
            if loaded_stats:
                # ä»æ–‡ä»¶æ¢å¤å·²æœ‰çš„ç»Ÿè®¡æ•°æ®
                await _restore_from_file(instance, loaded_stats)
            else:
                # é¦–æ¬¡è¿è¡Œæˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ‰§è¡Œå¤±è´¥è®¡æ•°è¿ç§»
                await _migrate_failure_counts_to_bayesian(instance)
                
    except Exception as e:
        logger.error(f"Error restoring Bayesian stats: {e}")


async def _restore_from_file(instance: KeyManager, loaded_stats: dict):
    """ä»æ–‡ä»¶æ¢å¤è´å¶æ–¯ç»Ÿè®¡æ•°æ®ï¼ŒåŒæ—¶å¤„ç†æ–°å¢keysçš„è¿ç§»"""
    try:
        migrated_keys = []
        
        async with instance.bayesian_stats_lock:
            key_stats = loaded_stats.get("key_stats", {})
            for key in instance.api_keys:
                if key in key_stats:
                    # æ¢å¤å·²æœ‰çš„ç»Ÿè®¡æ•°æ®
                    instance.key_stats[key] = tuple(key_stats[key])
                else:
                    # æ–°å¢çš„keyéœ€è¦è¿ç§»å¤±è´¥è®¡æ•°
                    failure_count = instance.key_failure_counts.get(key, 0)
                    if failure_count > 0:
                        alpha = settings.BAYESIAN_ALPHA_PRIOR
                        beta = settings.BAYESIAN_BETA_PRIOR + failure_count
                        instance.key_stats[key] = (alpha, beta)
                        migrated_keys.append(f"{redact_key_for_logging(key)}(failures: {failure_count})")
                        logger.info(f"Migrated failure count for new key {redact_key_for_logging(key)}: {failure_count} failures -> beta={beta}")
        
        async with instance.vertex_bayesian_stats_lock:
            vertex_key_stats = loaded_stats.get("vertex_key_stats", {})
            for key in instance.vertex_api_keys:
                if key in vertex_key_stats:
                    # æ¢å¤å·²æœ‰çš„ç»Ÿè®¡æ•°æ®
                    instance.vertex_key_stats[key] = tuple(vertex_key_stats[key])
                else:
                    # æ–°å¢çš„vertex keyéœ€è¦è¿ç§»å¤±è´¥è®¡æ•°
                    failure_count = instance.vertex_key_failure_counts.get(key, 0)
                    if failure_count > 0:
                        alpha = settings.BAYESIAN_ALPHA_PRIOR
                        beta = settings.BAYESIAN_BETA_PRIOR + failure_count
                        instance.vertex_key_stats[key] = (alpha, beta)
                        migrated_keys.append(f"Vertex-{redact_key_for_logging(key)}(failures: {failure_count})")
                        logger.info(f"Migrated failure count for new Vertex key {redact_key_for_logging(key)}: {failure_count} failures -> beta={beta}")
        
        logger.info(f"Loaded Bayesian stats from file: {len(key_stats)} API keys, {len(vertex_key_stats)} Vertex keys")
        if migrated_keys:
            logger.info(f"Migrated failure counts for {len(migrated_keys)} new keys: {', '.join(migrated_keys)}")
    
    except Exception as e:
        logger.error(f"Error restoring from file: {e}")


async def _migrate_failure_counts_to_bayesian(instance: KeyManager):
    """é¦–æ¬¡è¿è¡Œæ—¶ï¼Œå°†ç°æœ‰å¤±è´¥è®¡æ•°å®Œæ•´è¿ç§»åˆ°è´å¶æ–¯ç»Ÿè®¡ç³»ç»Ÿ"""
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨è¿ç§»
    if not settings.BAYESIAN_MIGRATION_ENABLED:
        logger.info("ğŸš« Bayesian migration is disabled, using default initialization")
        return
    
    try:
        migrated_api_keys = []
        migrated_vertex_keys = []
        
        # å¤‡ä»½åŸå§‹ç»Ÿè®¡æ•°æ®
        if settings.BAYESIAN_MIGRATION_BACKUP:
            await _backup_original_stats(instance)
        
        logger.info("ğŸ”„ Starting failure count migration to Bayesian statistics...")
        
        async with instance.bayesian_stats_lock:
            for key in instance.api_keys:
                failure_count = instance.key_failure_counts.get(key, 0)
                alpha = settings.BAYESIAN_ALPHA_PRIOR
                beta = settings.BAYESIAN_BETA_PRIOR + failure_count
                
                # è¦†ç›–åˆå§‹åŒ–æ—¶çš„é»˜è®¤å€¼
                instance.key_stats[key] = (alpha, beta)
                
                if failure_count > 0:
                    migrated_api_keys.append(f"{redact_key_for_logging(key)}({failure_count})")
                    logger.debug(f"Migrated API key {redact_key_for_logging(key)}: {failure_count} failures -> alpha={alpha}, beta={beta}")
        
        async with instance.vertex_bayesian_stats_lock:
            for key in instance.vertex_api_keys:
                failure_count = instance.vertex_key_failure_counts.get(key, 0)
                alpha = settings.BAYESIAN_ALPHA_PRIOR
                beta = settings.BAYESIAN_BETA_PRIOR + failure_count
                
                # è¦†ç›–åˆå§‹åŒ–æ—¶çš„é»˜è®¤å€¼
                instance.vertex_key_stats[key] = (alpha, beta)
                
                if failure_count > 0:
                    migrated_vertex_keys.append(f"{redact_key_for_logging(key)}({failure_count})")
                    logger.debug(f"Migrated Vertex key {redact_key_for_logging(key)}: {failure_count} failures -> alpha={alpha}, beta={beta}")
        
        logger.info("ğŸ”„ Completed initial failure count migration to Bayesian statistics")
        if migrated_api_keys:
            logger.info(f"ğŸ“Š Migrated {len(migrated_api_keys)} API keys with failures: {', '.join(migrated_api_keys)}")
        if migrated_vertex_keys:
            logger.info(f"ğŸ“Š Migrated {len(migrated_vertex_keys)} Vertex keys with failures: {', '.join(migrated_vertex_keys)}")
        
        # ç«‹å³ä¿å­˜è¿ç§»ç»“æœ
        instance._save_bayesian_stats("bayesian_key_stats.json")
        logger.info("ğŸ’¾ Saved migrated statistics to persistent storage")
    
    except Exception as e:
        logger.error(f"Error migrating failure counts: {e}")


async def _backup_original_stats(instance: KeyManager):
    """å¤‡ä»½åŸå§‹å¤±è´¥è®¡æ•°æ•°æ®"""
    try:
        backup_data = {
            "timestamp": time.time(),
            "migration_backup": True,
            "original_failure_counts": dict(instance.key_failure_counts),
            "original_vertex_failure_counts": dict(instance.vertex_key_failure_counts),
            "api_keys": list(instance.api_keys),
            "vertex_api_keys": list(instance.vertex_api_keys)
        }
        
        backup_filename = f"failure_counts_backup_{int(time.time())}.json"
        with open(backup_filename, 'w') as f:
            json.dump(backup_data, f, indent=2)
        
        logger.info(f"ğŸ’¾ Backed up original failure counts to {backup_filename}")
        
    except Exception as e:
        logger.error(f"Error backing up original stats: {e}")


async def _auto_save_bayesian_stats(instance: KeyManager):
    """è‡ªåŠ¨ä¿å­˜è´å¶æ–¯ç»Ÿè®¡æ•°æ®ï¼ˆå¦‚æœè·ç¦»ä¸Šæ¬¡ä¿å­˜è¶…è¿‡é—´éš”æ—¶é—´ï¼‰"""
    global _last_stats_save_time
    
    current_time = time.time()
    if current_time - _last_stats_save_time >= settings.BAYESIAN_STATS_DUMP_INTERVAL:
        try:
            instance._save_bayesian_stats("bayesian_key_stats.json")
            _last_stats_save_time = current_time
            logger.debug(f"Auto-saved Bayesian stats at {current_time}")
        except Exception as e:
            logger.error(f"Error auto-saving Bayesian stats: {e}")


async def save_bayesian_stats_on_shutdown():
    """åœ¨åº”ç”¨å…³é—­æ—¶å¼ºåˆ¶ä¿å­˜è´å¶æ–¯ç»Ÿè®¡æ•°æ®"""
    global _singleton_instance
    
    if _singleton_instance:
        try:
            _singleton_instance._save_bayesian_stats("bayesian_key_stats.json")
            logger.info("Saved Bayesian stats on shutdown")
        except Exception as e:
            logger.error(f"Error saving Bayesian stats on shutdown: {e}")
    else:
        logger.warning("No KeyManager instance to save stats from")
